adl.feature.ownerandgroup.enableupn = false
adl.http.timeout = -1
dfs.client.ignore.namenode.default.kms.uri = false
dfs.ha.fencing.ssh.connect-timeout = 30000
file.blocksize = 67108864
file.bytes-per-checksum = 512
file.client-write-packet-size = 65536
file.replication = 1
file.stream-buffer-size = 4096
fs.abfs.impl = org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.abfss.impl = org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.AbstractFileSystem.abfs.impl = org.apache.hadoop.fs.azurebfs.Abfs
fs.AbstractFileSystem.abfss.impl = org.apache.hadoop.fs.azurebfs.Abfss
fs.AbstractFileSystem.adl.impl = org.apache.hadoop.fs.adl.Adl
fs.AbstractFileSystem.file.impl = org.apache.hadoop.fs.local.LocalFs
fs.AbstractFileSystem.ftp.impl = org.apache.hadoop.fs.ftp.FtpFs
fs.AbstractFileSystem.gs.impl = com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
fs.AbstractFileSystem.har.impl = org.apache.hadoop.fs.HarFs
fs.AbstractFileSystem.hdfs.impl = org.apache.hadoop.fs.Hdfs
fs.AbstractFileSystem.s3a.impl = org.apache.hadoop.fs.s3a.S3A
fs.AbstractFileSystem.swebhdfs.impl = org.apache.hadoop.fs.SWebHdfs
fs.AbstractFileSystem.viewfs.impl = org.apache.hadoop.fs.viewfs.ViewFs
fs.AbstractFileSystem.wasb.impl = org.apache.hadoop.fs.azure.Wasb
fs.AbstractFileSystem.wasbs.impl = org.apache.hadoop.fs.azure.Wasbs
fs.AbstractFileSystem.webhdfs.impl = org.apache.hadoop.fs.WebHdfs
fs.adl.impl = org.apache.hadoop.fs.adl.AdlFileSystem
fs.adl.oauth2.access.token.provider.type = ClientCredential
fs.automatic.close = true
fs.azure.authorization.caching.enable = true
fs.azure.authorization = false
fs.azure.buffer.dir = /home/wanghao/Fuzzing4Ctest/fuzzing/hadoop/hadoop-common-project/hadoop-common/target/tmp/abfs
fs.azure.local.sas.key.mode = false
fs.azure.sas.expiry.period = 90d
fs.azure.saskey.usecontainersaskeyforallaccess = true
fs.azure.secure.mode = false
fs.azure.user.agent.prefix = unknown
fs.client.resolve.remote.symlinks = true
fs.client.resolve.topology.enabled = false
fs.defaultFS = file:///
fs.df.interval = 60000
fs.du.interval = 600000
fs.ftp.data.connection.mode = ACTIVE_LOCAL_DATA_CONNECTION_MODE
fs.ftp.host = 0.0.0.0
fs.ftp.host.port = 21
fs.ftp.impl = org.apache.hadoop.fs.ftp.FTPFileSystem
fs.ftp.password.localhost = password
fs.ftp.timeout = 0
fs.ftp.transfer.mode = BLOCK_TRANSFER_MODE
fs.ftp.user.localhost = user
fs.getspaceused.jitterMillis = 60000
fs.har.impl.disable.cache = true
fs.permissions.umask-mode = 022
fs.s3a.accesspoint.required = false
fs.s3a.assumed.role.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
fs.s3a.assumed.role.session.duration = 30m
fs.s3a.attempts.maximum = 20
fs.s3a.aws.credentials.provider = 
fs.s3a.block.size = 32M
fs.s3a.buffer.dir = /home/swang516/xlab/jqf-hadoop/hadoop/hadoop-common-project/hadoop-common/target/tmp/s3a
fs.s3a.change.detection.mode = server
fs.s3a.change.detection.source = etag
fs.s3a.change.detection.version.required = true
fs.s3a.committer.abort.pending.uploads = true
fs.s3a.committer.magic.enabled = true
fs.s3a.committer.name = file
fs.s3a.committer.staging.conflict-mode = append
fs.s3a.committer.staging.tmp.path = tmp/staging
fs.s3a.committer.staging.unique-filenames = true
fs.s3a.committer.threads = 8
fs.s3a.connection.establish.timeout = 5000
fs.s3a.connection.maximum = 96
fs.s3a.connection.request.timeout = 0
fs.s3a.connection.ssl.enabled = true
fs.s3a.connection.timeout = 200000
fs.s3a.downgrade.syncable.exceptions = true
fs.s3a.etag.checksum.enabled = false
fs.s3a.executor.capacity = 16
fs.s3a.fast.upload.active.blocks = 4
fs.s3a.fast.upload.buffer = disk
fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem
fs.s3a.list.version = 2
fs.s3a.max.total.tasks = 32
fs.s3a.metadatastore.authoritative = false
fs.s3a.metadatastore.fail.on.write.error = true
fs.s3a.metadatastore.impl = org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore
fs.s3a.metadatastore.metadata.ttl = 15m
fs.s3a.multiobjectdelete.enable = true
fs.s3a.multipart.purge.age = 86400
fs.s3a.multipart.purge = false
fs.s3a.multipart.size = 64M
fs.s3a.multipart.threshold = 128M
fs.s3a.paging.maximum = 5000
fs.s3a.path.style.access = false
fs.s3a.readahead.range = 64K
fs.s3a.retry.interval = 500ms
fs.s3a.retry.limit = 7
fs.s3a.retry.throttle.interval = 100ms
fs.s3a.retry.throttle.limit = 20
fs.s3a.s3guard.cli.prune.age = 86400000
fs.s3a.s3guard.consistency.retry.interval = 2s
fs.s3a.s3guard.consistency.retry.limit = 7
fs.s3a.s3guard.ddb.background.sleep = 25ms
fs.s3a.s3guard.ddb.max.retries = 9
fs.s3a.s3guard.ddb.table.capacity.read = 0
fs.s3a.s3guard.ddb.table.capacity.write = 0
fs.s3a.s3guard.ddb.table.create = false
fs.s3a.s3guard.ddb.table.sse.enabled = false
fs.s3a.s3guard.ddb.throttle.retry.interval = 100ms
fs.s3a.select.enabled = true
fs.s3a.select.errors.include.sql = false
fs.s3a.select.input.compression = none
fs.s3a.select.input.csv.comment.marker = #
fs.s3a.select.input.csv.field.delimiter = ,
fs.s3a.select.input.csv.header = none
fs.s3a.select.input.csv.quote.character = "
fs.s3a.select.input.csv.quote.escape.character = \\
fs.s3a.select.input.csv.record.delimiter = \n
fs.s3a.select.output.csv.field.delimiter = ,
fs.s3a.select.output.csv.quote.character = "
fs.s3a.select.output.csv.quote.escape.character = \\
fs.s3a.select.output.csv.quote.fields = always
fs.s3a.select.output.csv.record.delimiter = \n
fs.s3a.socket.recv.buffer = 8192
fs.s3a.socket.send.buffer = 8192
fs.s3a.ssl.channel.mode = default_jsse
fs.s3a.threads.keepalivetime = 60
fs.s3a.threads.max = 64
fs.swift.impl = org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.trash.checkpoint.interval = 0
fs.trash.interval = 0
fs.viewfs.overload.scheme.target.abfs.impl = org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.viewfs.overload.scheme.target.abfss.impl = org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.viewfs.overload.scheme.target.file.impl = org.apache.hadoop.fs.LocalFileSystem
fs.viewfs.overload.scheme.target.ftp.impl = org.apache.hadoop.fs.ftp.FTPFileSystem
fs.viewfs.overload.scheme.target.gs.impl = com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
fs.viewfs.overload.scheme.target.hdfs.impl = org.apache.hadoop.hdfs.DistributedFileSystem
fs.viewfs.overload.scheme.target.http.impl = org.apache.hadoop.fs.http.HttpFileSystem
fs.viewfs.overload.scheme.target.https.impl = org.apache.hadoop.fs.http.HttpsFileSystem
fs.viewfs.overload.scheme.target.o3fs.impl = org.apache.hadoop.fs.ozone.OzoneFileSystem
fs.viewfs.overload.scheme.target.ofs.impl = org.apache.hadoop.fs.ozone.RootedOzoneFileSystem
fs.viewfs.overload.scheme.target.oss.impl = org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem
fs.viewfs.overload.scheme.target.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem
fs.viewfs.overload.scheme.target.swebhdfs.impl = org.apache.hadoop.hdfs.web.SWebHdfsFileSystem
fs.viewfs.overload.scheme.target.swift.impl = org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.viewfs.overload.scheme.target.wasb.impl = org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.viewfs.overload.scheme.target.webhdfs.impl = org.apache.hadoop.hdfs.web.WebHdfsFileSystem
fs.viewfs.rename.strategy = SAME_MOUNTPOINT
fs.wasb.impl = org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasbs.impl = org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure
ftp.blocksize = 67108864
ftp.bytes-per-checksum = 512
ftp.client-write-packet-size = 65536
ftp.replication = 3
ftp.stream-buffer-size = 4096
hadoop.caller.context.enabled = false
hadoop.caller.context.max.size = 128
hadoop.caller.context.signature.max.size = 40
hadoop.common.configuration.version = 3.0.0
hadoop.domainname.resolver.impl = org.apache.hadoop.net.DNSDomainNameResolver
hadoop.http.authentication.kerberos.keytab = /home/swang516/hadoop.keytab
hadoop.http.authentication.kerberos.principal = HTTP/_HOST@LOCALHOST
hadoop.http.authentication.signature.secret.file = /home/swang516/hadoop-http-auth-signature-secret
hadoop.http.authentication.simple.anonymous.allowed = true
hadoop.http.authentication.token.validity = 36000
hadoop.http.authentication.type = simple
hadoop.http.cross-origin.allowed-headers = X-Requested-With,Content-Type,Accept,Origin
hadoop.http.cross-origin.allowed-methods = GET,POST,HEAD
hadoop.http.cross-origin.allowed-origins = *
hadoop.http.cross-origin.enabled = false
hadoop.http.cross-origin.max-age = 1800
hadoop.http.filter.initializers = org.apache.hadoop.http.lib.StaticUserWebFilter
hadoop.http.idle_timeout.ms = 60000
hadoop.http.logs.enabled = true
hadoop.http.sni.host.check.enabled = false
hadoop.http.staticuser.user = dr.who
hadoop.jetty.logs.serve.aliases = true
hadoop.kerberos.keytab.login.autorenewal.enabled = false
hadoop.kerberos.kinit.command = kinit
hadoop.kerberos.min.seconds.before.relogin = 60
hadoop.metrics.jvm.use-thread-mxbean = false
hadoop.prometheus.endpoint.enabled = false
hadoop.registry.jaas.context = Client
hadoop.registry.secure = false
hadoop.registry.system.acls = sasl:yarn@,
hadoop.registry.zk.connection.timeout.ms = 15000
hadoop.registry.zk.quorum = localhost:2181
hadoop.registry.zk.retry.ceiling.ms = 60000
hadoop.registry.zk.retry.interval.ms = 1000
hadoop.registry.zk.retry.times = 5
hadoop.registry.zk.root = /registry
hadoop.registry.zk.session.timeout.ms = 60000
hadoop.rpc.protection = authentication
hadoop.rpc.socket.factory.class.default = org.apache.hadoop.net.StandardSocketFactory
hadoop.security.authentication = simple
hadoop.security.authorization = false
hadoop.security.auth_to_local.mechanism = hadoop
hadoop.security.credential.clear-text-fallback = true
hadoop.security.crypto.buffer.size = 8192
hadoop.security.crypto.cipher.suite = AES/CTR/NoPadding
hadoop.security.crypto.codec.classes.aes.ctr.nopadding = org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,
hadoop.security.dns.log-slow-lookups.enabled = false
hadoop.security.dns.log-slow-lookups.threshold.ms = 1000
hadoop.security.group.mapping.ldap.connection.timeout.ms = 60000
hadoop.security.group.mapping.ldap.conversion.rule = none
hadoop.security.group.mapping.ldap.directory.search.timeout = 10000
hadoop.security.group.mapping.ldap.num.attempts = 3
hadoop.security.group.mapping.ldap.num.attempts.before.failover = 3
hadoop.security.group.mapping.ldap.posix.attr.gid.name = gidNumber
hadoop.security.group.mapping.ldap.posix.attr.uid.name = uidNumber
hadoop.security.group.mapping.ldap.read.timeout.ms = 60000
hadoop.security.group.mapping.ldap.search.attr.group.name = cn
hadoop.security.group.mapping.ldap.search.attr.member = member
hadoop.security.group.mapping.ldap.search.filter.group = (objectClass=group)
hadoop.security.group.mapping.ldap.search.filter.user = (&(objectClass=user)(sAMAccountName={0}))
hadoop.security.group.mapping.ldap.search.group.hierarchy.levels = 0
hadoop.security.group.mapping.ldap.ssl = false
hadoop.security.group.mapping = org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
hadoop.security.group.mapping.providers.combined = true
hadoop.security.groups.cache.background.reload = false
hadoop.security.groups.cache.background.reload.threads = 3
hadoop.security.groups.cache.secs = 300
hadoop.security.groups.cache.warn.after.ms = 5000
hadoop.security.groups.negative-cache.secs = 30
hadoop.security.groups.shell.command.timeout = 0s
hadoop.security.instrumentation.requires.admin = false
hadoop.security.java.secure.random.algorithm = SHA1PRNG
hadoop.security.key.default.bitlength = 128
hadoop.security.key.default.cipher = AES/CTR/NoPadding
hadoop.security.kms.client.authentication.retry-count = 1
hadoop.security.kms.client.encrypted.key.cache.expiry = 43200000
hadoop.security.kms.client.encrypted.key.cache.low-watermark = 0.3f
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads = 2
hadoop.security.kms.client.encrypted.key.cache.size = 500
hadoop.security.kms.client.failover.sleep.base.millis = 100
hadoop.security.kms.client.failover.sleep.max.millis = 2000
hadoop.security.kms.client.timeout = 60
hadoop.security.random.device.file.path = /dev/urandom
hadoop.security.secure.random.impl = org.apache.hadoop.crypto.random.OpensslSecureRandom
hadoop.security.sensitive-config-keys = 
hadoop.security.token.service.use_ip = true
hadoop.security.uid.cache.secs = 14400
hadoop.service.shutdown.timeout = 30s
hadoop.shell.missing.defaultFs.warning = false
hadoop.shell.safely.delete.limit.num.files = 100
hadoop.ssl.client.conf = ssl-client.xml
hadoop.ssl.enabled.protocols = TLSv1.2
hadoop.ssl.hostname.verifier = DEFAULT
hadoop.ssl.keystores.factory.class = org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
hadoop.ssl.require.client.cert = false
hadoop.ssl.server.conf = ssl-server.xml
hadoop.system.tags = YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
hadoop.tags.system = YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
hadoop.tmp.dir = build/test
hadoop.user.group.static.mapping.overrides = dr.who=;
hadoop.util.hash.type = murmur
hadoop.workaround.non.threadsafe.getpwuid = true
hadoop.zk.acl = world:anyone:rwcda
hadoop.zk.num-retries = 1000
hadoop.zk.retry-interval-ms = 1000
hadoop.zk.timeout-ms = 10000
ha.failover-controller.active-standby-elector.zk.op.retries = 3
ha.failover-controller.cli-check.rpc-timeout.ms = 20000
ha.failover-controller.graceful-fence.connection.retries = 1
ha.failover-controller.graceful-fence.rpc-timeout.ms = 5000
ha.failover-controller.new-active.rpc-timeout.ms = 60000
ha.health-monitor.check-interval.ms = 1000
ha.health-monitor.connect-retry-interval.ms = 1000
ha.health-monitor.rpc.connect.max.retries = 1
ha.health-monitor.rpc-timeout.ms = 45000
ha.health-monitor.sleep-after-disconnect.ms = 1000
ha.zookeeper.acl = world:anyone:rwcda
ha.zookeeper.parent-znode = /hadoop-ha
ha.zookeeper.session-timeout.ms = 10000
io.bytes.per.checksum = 512
io.compression.codec.bzip2.library = system-native
io.erasurecode.codec.rs-legacy.rawcoders = rs-legacy_java
io.erasurecode.codec.rs.rawcoders = rs_native,rs_java
io.erasurecode.codec.xor.rawcoders = xor_native,xor_java
io.file.buffer.size = 4096
io.mapfile.bloom.error.rate = 0.005
io.mapfile.bloom.size = 1048576
io.map.index.interval = 128
io.map.index.skip = 0
io.seqfile.compress.blocksize = 1000000
io.seqfile.local.dir = /home/wanghao/Fuzzing4Ctest/fuzzing/hadoop/hadoop-common-project/hadoop-common/target/tmp/io/local
io.serializations = org.apache.hadoop.io.serializer.WritableSerialization,
io.skip.checksum.errors = false
ipc.client.bind.wildcard.addr = false
ipc.client.connection.maxidletime = 10000
ipc.client.connect.max.retries = 10
ipc.client.connect.max.retries.on.timeouts = 45
ipc.client.connect.retry.interval = 1000
ipc.client.connect.timeout = 20000
ipc.client.fallback-to-simple-auth-allowed = false
ipc.client.idlethreshold = 4000
ipc.client.kill.max = 10
ipc.client.low-latency = false
ipc.client.ping = true
ipc.client.rpc-timeout.ms = 0
ipc.client.tcpnodelay = true
ipc.maximum.data.length = 134217728
ipc.maximum.response.length = 134217728
ipc.ping.interval = 60000
ipc.[port_number].backoff.enable = false
ipc.[port_number].callqueue.impl = java.util.concurrent.LinkedBlockingQueue
ipc.[port_number].cost-provider.impl = org.apache.hadoop.ipc.DefaultCostProvider
ipc.[port_number].decay-scheduler.backoff.responsetime.enable = false
ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds = 10s,20s,30s,40s
ipc.[port_number].decay-scheduler.decay-factor = 0.5
ipc.[port_number].decay-scheduler.metrics.top.user.count = 10
ipc.[port_number].decay-scheduler.period-ms = 5000
ipc.[port_number].decay-scheduler.thresholds = 13,25,50
ipc.[port_number].faircallqueue.multiplexer.weights = 8,4,2,1
ipc.[port_number].identity-provider.impl = org.apache.hadoop.ipc.UserIdentityProvider
ipc.[port_number].scheduler.impl = org.apache.hadoop.ipc.DefaultRpcScheduler
ipc.[port_number].scheduler.priority.levels = 4
ipc.[port_number].weighted-cost.handler = 1
ipc.[port_number].weighted-cost.lockexclusive = 100
ipc.[port_number].weighted-cost.lockfree = 1
ipc.[port_number].weighted-cost.lockshared = 10
ipc.[port_number].weighted-cost.response = 1
ipc.server.listen.queue.size = 256
ipc.server.log.slow.rpc = false
ipc.server.max.connections = 0
ipc.server.purge.interval = 15
ipc.server.reuseaddr = true
net.topology.impl = org.apache.hadoop.net.NetworkTopology
net.topology.node.switch.mapping.impl = org.apache.hadoop.net.ScriptBasedMapping
net.topology.script.number.args = 100
nfs3.mountd.port = 4272
nfs3.server.port = 2079
nfs.exports.allowed.hosts = *
rpc.metrics.quantile.enable = false
rpc.metrics.timeunit = MILLISECONDS
seq.io.sort.factor = 100
seq.io.sort.mb = 100
testconfservlet.key = testval
tfile.fs.input.buffer.size = 262144
tfile.fs.output.buffer.size = 262144
tfile.io.chunk.size = 1048576
